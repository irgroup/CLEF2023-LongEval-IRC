{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMfrIiUTC4IQ"
      },
      "source": [
        "based on: https://github.com/vjeronymo2/pygaggle/blob/master/pygaggle/run/finetune_monot5.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5fCzOsnzpsM5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jueri/miniconda3/envs/LongEval/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2023-05-01 22:03:34.983692: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-01 22:03:35.501646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import jsonlines\n",
        "import argparse\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    TrainerCallback,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GPon7iyNSh3Q"
      },
      "outputs": [],
      "source": [
        "queries=pd.read_csv(\"../data/publish/English/Queries/train.tsv\", delimiter = \"\\t\", names=[\"idx\", \"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Zy2MHiqATYGB"
      },
      "outputs": [],
      "source": [
        "all_ids = queries.text.to_list()\n",
        "train_ids, validation_ids, test_ids = np.split(\n",
        "    all_ids, [int(0.6 * len(all_ids)), int(0.8 * len(all_ids))]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "05h28ZjxqhJZ"
      },
      "outputs": [],
      "source": [
        "################################\n",
        "base_model = 't5-base'\n",
        "# base_model = \"castorini/monoT5-base-msmarco\"\n",
        "triples_path = \"../data/passages.jsonl\"\n",
        "output_model_path = \"../data/models/monoT5-WT/train/checkpoints/\"\n",
        "save_every_n_steps = 1000\n",
        "logging_steps = 100\n",
        "per_device_train_batch_size = 6\n",
        "gradient_accumulation_steps = 16\n",
        "learning_rate = 3e-4  # original\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "31ATVG0VpvE-"
      },
      "outputs": [],
      "source": [
        "class MonoT5Dataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        text = f'Query: {sample[0]} Document: {sample[1]} Relevant:'\n",
        "        return {\n",
        "          'text': text,\n",
        "          'labels': sample[2],\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfzV_lSbqQC0",
        "outputId": "827999a1-3ab8-4b3b-a5f6-0e89f331a404"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f01491f30d0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda')\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QoN6Kl_pqWHu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jueri/miniconda3/envs/LongEval/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(base_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained('t5-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JhymLK2isLWN"
      },
      "outputs": [],
      "source": [
        "train_samples = []\n",
        "with open(triples_path, 'r', encoding=\"utf-8\") as fIn:\n",
        "    for num, line in enumerate(fIn):\n",
        "\n",
        "        if num > 6.4e5 * epochs:\n",
        "            break\n",
        "        if line == \"\\n\":\n",
        "            continue\n",
        "        line = json.loads(line)\n",
        "\n",
        "        # limit to train queries\n",
        "        if line[0] not in train_ids:\n",
        "          continue\n",
        "\n",
        "        train_samples.append((line[0], line[1], 'true'))\n",
        "        train_samples.append((line[0], line[2], 'false'))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3xOi7Cb88fTM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "28260"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HX3AYPbC8fWC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "27649"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(set(train_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_jH6L9-pUUGX"
      },
      "outputs": [],
      "source": [
        "train_samples = list(set(train_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yfUdn0OVvnQE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "27649"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pmXxwFCwsioL"
      },
      "outputs": [],
      "source": [
        "def smart_batching_collate_text_only(batch):\n",
        "    texts = [example['text'] for example in batch]\n",
        "    tokenized = tokenizer(texts, padding=True, truncation='longest_first', return_tensors='pt', max_length=512)\n",
        "    tokenized['labels'] = tokenizer([example['labels'] for example in batch], return_tensors='pt')['input_ids']\n",
        "\n",
        "    for name in tokenized:\n",
        "        tokenized[name] = tokenized[name].to(device)\n",
        "\n",
        "    return tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gAoChztg6rU8"
      },
      "outputs": [],
      "source": [
        "dataset_train = MonoT5Dataset(train_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZuW77sVF61rK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jueri/miniconda3/envs/LongEval/lib/python3.8/site-packages/transformers/training_args.py:1243: FutureWarning: `--adafactor` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--optim adafactor` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=output_model_path,\n",
        "        do_train=True,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps =save_every_n_steps, \n",
        "        logging_steps=logging_steps,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=5e-5,\n",
        "        num_train_epochs=1,\n",
        "        warmup_steps=0,\n",
        "        # warmup_steps=1000,\n",
        "        adafactor=True,\n",
        "        seed=1,\n",
        "        disable_tqdm=False,\n",
        "        load_best_model_at_end=False,\n",
        "        predict_with_generate=True,\n",
        "        dataloader_pin_memory=False,\n",
        "        remove_unused_columns=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "71aOXuqU7GNR"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=train_args,\n",
        "    train_dataset=dataset_train,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=smart_batching_collate_text_only,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "SKSZBb4w7KcP",
        "outputId": "c58955b1-164f-4eef-ade0-4b7b67309307"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Trainer is attempting to log a value of \"{'summarization': {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 200, 'min_length': 30, 'no_repeat_ngram_size': 3, 'num_beams': 4, 'prefix': 'summarize: '}, 'translation_en_to_de': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to German: '}, 'translation_en_to_fr': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to French: '}, 'translation_en_to_ro': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to Romanian: '}}\" for key \"task_specific_params\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute. You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and avoid this message.\n",
            " 35%|███▍      | 100/288 [02:42<04:47,  1.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6259, 'learning_rate': 0.00019583333333333331, 'epoch': 0.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 200/288 [05:28<02:11,  1.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3517, 'learning_rate': 9.166666666666667e-05, 'epoch': 0.69}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 288/288 [08:02<00:00,  1.67s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 482.7091, 'train_samples_per_second': 57.279, 'train_steps_per_second': 0.597, 'train_loss': 0.44365525907940334, 'epoch': 1.0}\n"
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "\n",
        "trainer.save_model(output_model_path.replace(\"checkpoints\", \"monoT5-WT\"))\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.16 ('LongEval')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "fb915a841106d17bab6c0e433d807167dd40c228f7e5885474f2fcf8f68fafdf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
